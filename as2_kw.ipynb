{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pickle\n","import time\n","import numpy\n","import urllib\n","import scipy.optimize\n","import random\n","from collections import defaultdict\n","from collections import Counter\n","import string\n","from sklearn import (\n","    linear_model,\n","    naive_bayes,\n","    svm,\n","    metrics,\n","    tree,\n","    neighbors,\n","    decomposition,\n",")\n","import ast\n","import importlib\n","import numpy as np\n","from scipy.spatial import distance\n","import random\n","import nltk\n","import pandas as pd\n",""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import sys, os\n","\n","sys.path.insert(0, os.path.abspath(\"../cse258_hw/\"))\n","\n",""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<module 'as2_analysis_utils' from '/Users/kwkarlwang/Desktop/Master/cse258/assignment2/as2_analysis_utils.py'>"]},"metadata":{},"execution_count":3}],"source":["import as2_analysis_utils as as2_analysis\n","\n","importlib.reload(as2_analysis)\n","\n",""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["data_all = list(as2_analysis.parseData_line(\"renttherunway_final_data.json\"))\n","\n",""]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def extract_features(data_all, key):\n","    \"\"\"\n","    return data of particualr key\n","    \"\"\"\n","    return [d[key] for d in data_all]\n","\n",""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["rating_all = extract_features(data_all, \"rating\")\n","\n",""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["null_count = [i == \"null\" for i in rating_all]\n","\n",""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0004258766827322586"]},"metadata":{},"execution_count":8}],"source":["sum(null_count) / len(null_count)\n","\n",""]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["fit_feedbadk = extract_features(data_all, \"fit\")\n","\n",""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["fit_feedbadk_freq = Counter(fit_feedbadk)\n","\n",""]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["fit_feedbadk_prct = {key: val / len(data_all) for key, val in fit_feedbadk_freq.items()}\n","\n",""]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'fit': 0.7377949975070633,\n"," 'small': 0.13388628053847432,\n"," 'large': 0.12831872195446237}"]},"metadata":{},"execution_count":12}],"source":["fit_feedbadk_prct\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Count word frequency"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# top words in fit reviews\n","fit_reviews_all = [\n","    d[\"review_text\"] + \" \" + d[\"review_summary\"] for d in data_all if d[\"fit\"] == \"fit\"\n","]\n","small_reviews_all = [\n","    d[\"review_text\"] + \" \" + d[\"review_summary\"]\n","    for d in data_all\n","    if d[\"fit\"] == \"small\"\n","]\n","large_reviews_all = [\n","    d[\"review_text\"] + \" \" + d[\"review_summary\"]\n","    for d in data_all\n","    if d[\"fit\"] == \"large\"\n","]\n","\n",""]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def tokenize_data_sets(dataset, n=1):\n","    \"\"\"\n","    Each item of dataset is a str\n","    \"\"\"\n","    return [as2_analysis.tokenize_paragraph(d, n=n) for d in dataset]\n","\n",""]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# tokenize dataset\n","fit_tokenized_all = tokenize_data_sets(fit_reviews_all)\n","small_tokenized_all = tokenize_data_sets(small_reviews_all)\n","large_tokenized_all = tokenize_data_sets(large_reviews_all)\n","\n",""]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["fit_word_count = as2_analysis.count_word_freq(fit_tokenized_all)\n","small_word_count = as2_analysis.count_word_freq(small_tokenized_all)\n","large_word_count = as2_analysis.count_word_freq(large_tokenized_all)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Get most frequent adjective"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def top_n_adj(word_count_dict):\n","    ## get the most common ajectives\n","    adj_count = {\n","        key: word_count_dict[key]\n","        for key in word_count_dict\n","        if nltk.pos_tag([key])[0][1][0] == \"J\"\n","    }\n","\n","    # sort them into pairs\n","    adj_count_pair = [(count, key) for key, count in adj_count.items()]\n","\n","    # sort\n","    adj_count_pair.sort(reverse=True)\n","\n","    return adj_count_pair\n","\n",""]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["time consume: 11.68418002128601s\n"]}],"source":["start = time.time()\n","top_small_adj = top_n_adj(small_word_count)\n","\n","\n","top_big_adj = top_n_adj(large_word_count)\n","\n","\n","## get the most common ajectives\n","adj_fit_count = {\n","    key: fit_word_count[key]\n","    for key in fit_word_count\n","    if nltk.pos_tag([key])[0][1][0] == \"J\"\n","}\n","\n","end = time.time()\n","print(f\"time consume: {end-start}s\")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["adj_fit_count_pair = [(count, key) for key, count in adj_fit_count.items()]\n","\n",""]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["adj_fit_count_pair.sort(reverse=True)\n","\n",""]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(60010, 'great'),\n"," (48689, 'comfortable'),\n"," (39921, 'little'),\n"," (22857, 'many'),\n"," (18916, 'true'),\n"," (17789, 'short'),\n"," (15455, 'black'),\n"," (13260, 'gorgeous'),\n"," (12799, 'much'),\n"," (11649, 'nice')]"]},"metadata":{},"execution_count":21}],"source":["adj_fit_count_pair[:10]\n","\n",""]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(11137, 'small'),\n"," (9126, 'great'),\n"," (8424, 'little'),\n"," (5654, 'comfortable'),\n"," (4705, 'short'),\n"," (3386, 'many'),\n"," (2854, 'gorgeous'),\n"," (2675, 'overall'),\n"," (2623, 'much'),\n"," (2620, 'other')]"]},"metadata":{},"execution_count":22}],"source":["top_small_adj[:10]\n","\n",""]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(9276, 'great'),\n"," (8972, 'little'),\n"," (8489, 'large'),\n"," (7584, 'comfortable'),\n"," (7517, 'big'),\n"," (3201, 'small'),\n"," (3011, 'many'),\n"," (2654, 'much'),\n"," (2602, 'overall'),\n"," (2406, 'smaller')]"]},"metadata":{},"execution_count":23}],"source":["top_big_adj[:10]\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Try bigram and trigram"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["def top_n_adj_n_gram(word_count_dict, adj_set=None):\n","\n","    # if set of adj is given\n","    if adj_set:\n","        adj_count = {\n","            key: word_count_dict[key]\n","            for key in word_count_dict\n","            # if either of the word is an adj, preserve\n","            if key in adj_set\n","        }\n","    else:\n","        ## get the most common ajectives, might be slow\n","        adj_count = {\n","            key: word_count_dict[key]\n","            for key in word_count_dict\n","            # if either of the word is an adj, preserve\n","            if any(\n","                wordType[0] == \"J\"\n","                for _, wordType in nltk.pos_tag(key if type(key) == tuple else (key,))\n","            )\n","        }\n","\n","    # sort them into pairs\n","    adj_count_pair = list(adj_count.items())\n","\n","    # # sort\n","    adj_count_pair.sort(reverse=True, key=lambda x: x[1])\n","\n","    return adj_count_pair\n","\n",""]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["def top_adj_pipeline(fit_reviews_all, small_reviews_all, large_reviews_all, n=2):\n","\n","    fit_tokenized_all = tokenize_data_sets(fit_reviews_all, n)\n","    small_tokenized_all = tokenize_data_sets(small_reviews_all, n)\n","    large_tokenized_all = tokenize_data_sets(large_reviews_all, n)\n","\n","    fit_word_count = as2_analysis.count_word_freq(fit_tokenized_all)\n","    small_word_count = as2_analysis.count_word_freq(small_tokenized_all)\n","    large_word_count = as2_analysis.count_word_freq(large_tokenized_all)\n","\n","    # all adj for fit, small, and large\n","    fit_adj, small_adj, large_adj = None, None, None\n","\n","    # load if the pickle exists\n","    if os.path.exists(f\"{n}-grams_adj\"):\n","        with open(f\"{n}-grams_adj\", \"rb\") as f:\n","            fit_adj, small_adj, large_adj = pickle.load(f)\n","\n","    start = time.time()\n","    top_fit_grams = top_n_adj_n_gram(fit_word_count, fit_adj)\n","    top_small_grams = top_n_adj_n_gram(small_word_count, small_adj)\n","    top_large_grams = top_n_adj_n_gram(large_word_count, large_adj)\n","    end = time.time()\n","    print(f\"time consume: {end-start}s\")\n","    return top_fit_grams, top_small_grams, top_large_grams\n","\n",""]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def write_adj_to_file(n):\n","    # get all the adj from the entire data\n","    top_fit_grams, top_small_grams, top_large_grams = top_adj_pipeline(\n","        fit_reviews_all, small_reviews_all, large_reviews_all, n\n","    )\n","    # make into sets\n","    a, b, c = (\n","        {word for word, _ in top_fit_grams},\n","        {word for word, _ in top_small_grams},\n","        {word for word, _ in top_large_grams},\n","    )\n","    # write to file\n","    with open(f\"./{n}-grams_adj\", \"wb\") as f:\n","        pickle.dump((a, b, c), f)\n","\n",""]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["time consume: 0.3970160484313965s\n"]}],"source":["top_fit_grams, top_small_grams, top_large_grams = top_adj_pipeline(\n","    fit_reviews_all, small_reviews_all, large_reviews_all, 2\n",")"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('a', 'little'), 37021),\n"," (('was', 'perfect'), 21087),\n"," (('many', 'compliments'), 18749),\n"," (('true', 'to'), 17248),\n"," (('so', 'i'), 16506),\n"," (('so', 'many'), 14559),\n"," (('comfortable', 'and'), 12829),\n"," (('very', 'comfortable'), 11175),\n"," (('the', 'top'), 9822),\n"," (('a', 'great'), 9118),\n"," (('great', 'dress'), 8835),\n"," (('able', 'to'), 7224),\n"," (('easy', 'to'), 7027),\n"," (('was', 'great'), 7012),\n"," (('great', 'for'), 6568),\n"," (('beautiful', 'dress'), 6409),\n"," (('the', 'only'), 6378),\n"," (('and', 'comfortable'), 5396),\n"," (('black', 'tie'), 5095),\n"," (('was', 'comfortable'), 5011)]"]},"metadata":{},"execution_count":28}],"source":["\n","top_fit_grams[:20]"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('a', 'little'), 7847),\n"," (('so', 'i'), 4534),\n"," (('many', 'compliments'), 2749),\n"," (('was', 'perfect'), 2652),\n"," (('runs', 'small'), 2557),\n"," (('the', 'top'), 2184),\n"," (('so', 'many'), 2041),\n"," (('able', 'to'), 1758),\n"," (('beautiful', 'dress'), 1729),\n"," (('small', 'i'), 1694),\n"," (('great', 'dress'), 1598),\n"," (('too', 'tight'), 1484),\n"," (('a', 'great'), 1465),\n"," (('too', 'short'), 1205),\n"," (('comfortable', 'and'), 1201),\n"," (('too', 'small'), 1119),\n"," (('normally', 'wear'), 1107),\n"," (('very', 'comfortable'), 1092),\n"," (('small', 'and'), 1053),\n"," (('little', 'small'), 1046)]"]},"metadata":{},"execution_count":29}],"source":["top_small_grams[:20]"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(('a', 'little'), 8558),\n"," (('so', 'i'), 3870),\n"," (('was', 'perfect'), 2582),\n"," (('the', 'top'), 2401),\n"," (('many', 'compliments'), 2366),\n"," (('too', 'big'), 2096),\n"," (('comfortable', 'and'), 2026),\n"," (('runs', 'large'), 1842),\n"," (('so', 'many'), 1829),\n"," (('very', 'comfortable'), 1735),\n"," (('little', 'big'), 1542),\n"," (('a', 'great'), 1539),\n"," (('great', 'dress'), 1404),\n"," (('able', 'to'), 1390),\n"," (('large', 'i'), 1381),\n"," (('beautiful', 'dress'), 1189),\n"," (('little', 'large'), 1174),\n"," (('great', 'for'), 1156),\n"," (('easy', 'to'), 1057),\n"," (('a', 'small'), 976)]"]},"metadata":{},"execution_count":30}],"source":["top_large_grams[:20]\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Feature Engineering"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["fit_data = [d for d in data_all if d[\"fit\"] == \"fit\"]\n","small_data = [d for d in data_all if d[\"fit\"] == \"small\"]\n","large_data = [d for d in data_all if d[\"fit\"] == \"large\"]\n","data_balanced = random.sample(fit_data, k=len(large_data)) + small_data + large_data"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["random.shuffle(data_balanced)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["data_size = len(data_balanced)\n","valid_percent = 0.2\n","test_percent = 0.2\n","\n",""]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["\n","data_train = data_balanced[: int(data_size * (1 - valid_percent - test_percent))]\n","data_valid = data_balanced[\n","    int(data_size * (1 - valid_percent - test_percent)) : int(\n","        data_size * (1 - test_percent)\n","    )\n","]\n","data_test = data_balanced[int(data_size * (1 - test_percent)) :]\n",""]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["def extract_review(data):\n","    fit_reviews = [\n","        d[\"review_text\"] + \" \" + d[\"review_summary\"] for d in data if d[\"fit\"] == \"fit\"\n","    ]\n","    small_reviews = [\n","        d[\"review_text\"] + \" \" + d[\"review_summary\"]\n","        for d in data\n","        if d[\"fit\"] == \"small\"\n","    ]\n","    large_reviews = [\n","        d[\"review_text\"] + \" \" + d[\"review_summary\"]\n","        for d in data\n","        if d[\"fit\"] == \"large\"\n","    ]\n","    return fit_reviews, small_reviews, large_reviews\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Try n-gram BoW logistic regression/navie bayes"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["fit_reviews_train, small_reviews_train, large_reviews_train = extract_review(data_train)\n",""]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["time consume: 0.005340099334716797s\n","unique top 1-grams: 18\n","time consume: 0.14866280555725098s\n","unique top 2-grams: 337\n","time consume: 0.6317377090454102s\n","unique top 3-grams: 1037\n","time consume: 1.2195277214050293s\n","unique top 4-grams: 2758\n","time consume: 1.7382757663726807s\n","unique top 5-grams: 5131\n","unique top grams: 9284\n"]}],"source":["ns = [1, 2, 3, 4, 5]\n","thresholds = [100, 500, 1000, 2000, 3000]\n","top_word_set = {\"small\", \"large\", \"big\"}\n","for n, threshold in zip(ns, thresholds):\n","    top_fit_grams, top_small_grams, top_large_grams = top_adj_pipeline(\n","        fit_reviews_train, small_reviews_train, large_reviews_train, n\n","    )\n","\n","    a, b, c = (\n","        {word for word, _ in top_fit_grams[:threshold]},\n","        {word for word, _ in top_small_grams[:threshold]},\n","        ({word for word, _ in top_large_grams[:threshold]}),\n","    )\n","    top_gram = (a | b | c) - (a & b & c)\n","    print(f\"unique top {n}-grams: {len(top_gram)}\")\n","    top_word_set |= top_gram\n","\n","print(f\"unique top grams: {len(top_word_set)}\")\n","\n","wordId = dict(zip(top_word_set, range(len(top_word_set))))"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["def feature(d, ns=[1]):\n","    feat = [0] * len(top_word_set)\n","    review = d[\"review_text\"] + \" \" + d[\"review_summary\"]\n","    p_list = []\n","    for n in ns:\n","        p_list.extend(as2_analysis.tokenize_paragraph(review, n=n))\n","    for word in p_list:\n","        if word not in top_word_set:\n","            continue\n","        # use BoW for now\n","        feat[wordId[word]] += 1\n","    return feat\n","\n",""]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["def encode_output(data):\n","    return [mapping[d[\"fit\"]] for d in data]\n","\n",""]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["mapping = {\"fit\": 0, \"small\": 1, \"large\": 2}\n","X_train = [feature(d, ns) for d in data_train]\n","y_train = encode_output(data_train)\n","X_valid = [feature(d, ns) for d in data_valid]\n","y_valid = encode_output(data_valid)\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Modeling"]},{"cell_type":"markdown","metadata":{},"source":[" Try the following numerous text classfication algorithm:\n"," \n","    - Navie Bayes\n","    - SVM\n","    - Logistic Regression\n","    - Decision Tree\n","    - KNN"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["models_data = {}"]},{"cell_type":"markdown","metadata":{},"source":[" ## Navie Bayes"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Navie Bayes classifcation report\n\n              precision    recall  f1-score   support\n\n           0       0.56      0.59      0.58      4975\n           1       0.61      0.69      0.65      5168\n           2       0.74      0.61      0.67      4896\n\n    accuracy                           0.63     15039\n   macro avg       0.64      0.63      0.63     15039\nweighted avg       0.64      0.63      0.63     15039\n\n"]}],"source":["model = naive_bayes.MultinomialNB()\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_valid)\n","print(\"Navie Bayes classifcation report\\n\")\n","print(metrics.classification_report(y_valid, y_pred))\n","models_data[\"naive bayes\"] = metrics.classification_report(\n","    y_valid, y_pred, output_dict=1\n",")\n",""]},{"cell_type":"markdown","metadata":{},"source":["## SVM\n","\n","Try the following kernel:\n","\n","    - linear\n","    - rbf\n","    - polynomial"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["SVM kernel=linear, C=0.01 classifcation report\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.54      0.76      0.63      4975\n","           1       0.76      0.61      0.68      5168\n","           2       0.76      0.60      0.67      4896\n","\n","    accuracy                           0.66     15039\n","   macro avg       0.69      0.66      0.66     15039\n","weighted avg       0.69      0.66      0.66     15039\n","\n","SVM kernel=linear, C=1 classifcation report\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.53      0.66      0.59      4975\n","           1       0.70      0.61      0.66      5168\n","           2       0.68      0.61      0.64      4896\n","\n","    accuracy                           0.63     15039\n","   macro avg       0.64      0.63      0.63     15039\n","weighted avg       0.64      0.63      0.63     15039\n","\n","SVM kernel=linear, C=10 classifcation report\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.53      0.64      0.58      4975\n","           1       0.68      0.60      0.64      5168\n","           2       0.67      0.60      0.63      4896\n","\n","    accuracy                           0.62     15039\n","   macro avg       0.63      0.62      0.62     15039\n","weighted avg       0.63      0.62      0.62     15039\n","\n","SVM kernel=linear, C=100 classifcation report\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.52      0.66      0.58      4975\n","           1       0.69      0.58      0.63      5168\n","           2       0.67      0.59      0.62      4896\n","\n","    accuracy                           0.61     15039\n","   macro avg       0.62      0.61      0.61     15039\n","weighted avg       0.62      0.61      0.61     15039\n","\n"]}],"source":["regs = [0.01, 1, 10, 100]\n","degs = [2, 3, 4]\n","models = {f\"kernel=linear, C={reg}\": svm.LinearSVC(C=reg) for reg in regs}\n","# models.update(\n","#     {f\"kernel=rbf, C={reg}\": svm.SVC(kernel=\"rbf\", C=reg, gamma=\"auto\") for reg in regs}\n","# )\n","# models.update(\n","#     {\n","#         f\"kernel=poly, C={reg}, deg={deg}\": svm.SVC(kernel=\"poly\", C=reg, gamma=\"auto\")\n","#         for reg in regs\n","#         for deg in degs\n","#     }\n","# )\n","for desc, model in models.items():\n","    if f\"svm, {desc}\" in models_data:\n","        continue\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_valid)\n","    print(f\"SVM {desc} classifcation report\\n\")\n","    print(metrics.classification_report(y_valid, y_pred))\n","    models_data[f\"svm, {desc}\"] = metrics.classification_report(\n","        y_valid, y_pred, output_dict=1\n","    )\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Logistic Regression"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression C=0.01, weights=None classifcation report\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.51      0.80      0.63      4975\n","           1       0.77      0.56      0.64      5168\n","           2       0.77      0.55      0.64      4896\n","\n","    accuracy                           0.64     15039\n","   macro avg       0.68      0.64      0.64     15039\n","weighted avg       0.68      0.64      0.64     15039\n","\n","Logistic Regression C=1, weights=None classifcation report\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.54      0.69      0.61      4975\n","           1       0.73      0.62      0.67      5168\n","           2       0.71      0.63      0.66      4896\n","\n","    accuracy                           0.65     15039\n","   macro avg       0.66      0.65      0.65     15039\n","weighted avg       0.66      0.65      0.65     15039\n","\n","Logistic Regression C=10, weights=None classifcation report\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.54      0.65      0.59      4975\n","           1       0.70      0.62      0.65      5168\n","           2       0.68      0.62      0.65      4896\n","\n","    accuracy                           0.63     15039\n","   macro avg       0.64      0.63      0.63     15039\n","weighted avg       0.64      0.63      0.63     15039\n","\n","Logistic Regression C=100, weights=None classifcation report\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.53      0.65      0.59      4975\n","           1       0.70      0.61      0.65      5168\n","           2       0.68      0.62      0.65      4896\n","\n","    accuracy                           0.63     15039\n","   macro avg       0.64      0.63      0.63     15039\n","weighted avg       0.64      0.63      0.63     15039\n","\n"]}],"source":["\n","regs = [0.01, 1, 10, 100]\n","weights = [None]\n","models = {\n","    f\"C={reg}, weights={weight}\": linear_model.LogisticRegression(\n","        C=reg, class_weight=weight\n","    )\n","    for reg in regs\n","    for weight in weights\n","}\n","\n","\n","for desc, model in models.items():\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_valid)\n","    print(f\"Logistic Regression {desc} classifcation report\\n\")\n","    print(metrics.classification_report(y_valid, y_pred))\n","    models_data[f\"logistic regression, {desc}\"] = metrics.classification_report(\n","        y_valid, y_pred, output_dict=1\n","    )\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Decision Tree"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["# depths = [50, 100, 200]\n","# models = {\n","#     f\"max_depth={depth}\": tree.DecisionTreeClassifier(max_depth=depth)\n","#     for depth in depths\n","# }\n","\n","\n","# for desc, model in models.items():\n","#     model.fit(X_train, y_train)\n","#     y_pred = model.predict(X_valid)\n","#     print(f\"Decision Tree {desc} classifcation report\\n\")\n","#     print(metrics.classification_report(y_valid, y_pred))\n","#     models_data[f\"decision tree, {desc}\"] = metrics.classification_report(\n","#         y_valid, y_pred, output_dict=1\n","#     )\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## K-nearest neighbor"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["# neighbor_nums = [1, 5, 10, 20, 50, 100]\n","# models = {\n","#     f\"n-neighbor={neighbor_num}\": neighbors.KNeighborsClassifier(\n","#         n_neighbors=neighbor_num\n","#     )\n","#     for neighbor_num in neighbor_nums\n","# }\n","# for desc, model in models.items():\n","#     model.fit(X_train, y_train)\n","#     y_pred = model.predict(X_valid)\n","#     print(f\"KNN {desc} classifcation report\\n\")\n","#     print(metrics.classification_report(y_valid, y_pred))\n","#     models_data[f\"knn, {desc}\"] = metrics.classification_report(\n","#         y_valid, y_pred, output_dict=1\n","#     )\n","\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}
