{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Charlie/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import string\n",
    "from sklearn import linear_model\n",
    "import ast\n",
    "import importlib\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import random\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'as2_analysis_utils' from '/Users/Charlie/Documents/Work_School/UCSD/Grad/Fall_2020/CSE258/assignment2/as2_analysis_utils.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import as2_analysis_utils as as2_analysis\n",
    "\n",
    "importlib.reload(as2_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = list(as2_analysis.parseData_line(\"renttherunway_final_data.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data_all, key):\n",
    "    \"\"\"\n",
    "    return data of particualr key\n",
    "    \"\"\"\n",
    "    return [d[key] for d in data_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Count word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top words in fit reviews\n",
    "fit_reviews_all = [\n",
    "    d[\"review_text\"] + \" \" + d[\"review_summary\"] for d in data_all if d[\"fit\"] == \"fit\"\n",
    "]\n",
    "small_reviews_all = [\n",
    "    d[\"review_text\"] + \" \" + d[\"review_summary\"]\n",
    "    for d in data_all\n",
    "    if d[\"fit\"] == \"small\"\n",
    "]\n",
    "large_reviews_all = [\n",
    "    d[\"review_text\"] + \" \" + d[\"review_summary\"]\n",
    "    for d in data_all\n",
    "    if d[\"fit\"] == \"large\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data_sets(dataset, n=1):\n",
    "    \"\"\"\n",
    "    Each item of dataset is a str\n",
    "    \"\"\"\n",
    "    return [as2_analysis.tokenize_paragraph(d, n=n) for d in dataset]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize dataset\n",
    "fit_tokenized_all = tokenize_data_sets(fit_reviews_all)\n",
    "small_tokenized_all = tokenize_data_sets(small_reviews_all)\n",
    "large_tokenized_all = tokenize_data_sets(large_reviews_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_word_count = as2_analysis.count_word_freq(fit_tokenized_all)\n",
    "small_word_count = as2_analysis.count_word_freq(small_tokenized_all)\n",
    "large_word_count = as2_analysis.count_word_freq(large_tokenized_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Get most frequent adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_adj(word_count_dict):\n",
    "    ## get the most common ajectives\n",
    "    adj_count = {\n",
    "        key: word_count_dict[key]\n",
    "        for key in word_count_dict\n",
    "        if nltk.pos_tag([key])[0][1][0] == \"J\"\n",
    "    }\n",
    "\n",
    "    # sort them into pairs\n",
    "    adj_count_pair = [(count, key) for key, count in adj_count.items()]\n",
    "\n",
    "    # sort\n",
    "    adj_count_pair.sort(reverse=True)\n",
    "\n",
    "    return adj_count_pair\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time consume: 12.169735193252563s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "top_small_adj = top_n_adj(small_word_count)\n",
    "\n",
    "\n",
    "top_big_adj = top_n_adj(large_word_count)\n",
    "\n",
    "\n",
    "## get the most common ajectives\n",
    "adj_fit_count = {\n",
    "    key: fit_word_count[key]\n",
    "    for key in fit_word_count\n",
    "    if nltk.pos_tag([key])[0][1][0] == \"J\"\n",
    "}\n",
    "\n",
    "end = time.time()\n",
    "print(f\"time consume: {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_fit_count_pair = [(count, key) for key, count in adj_fit_count.items()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_fit_count_pair.sort(reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(60010, 'great'),\n",
       " (48689, 'comfortable'),\n",
       " (39921, 'little'),\n",
       " (22857, 'many'),\n",
       " (18916, 'true'),\n",
       " (17789, 'short'),\n",
       " (15455, 'black'),\n",
       " (13260, 'gorgeous'),\n",
       " (12799, 'much'),\n",
       " (11649, 'nice')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_fit_count_pair[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11137, 'small'),\n",
       " (9126, 'great'),\n",
       " (8424, 'little'),\n",
       " (5654, 'comfortable'),\n",
       " (4705, 'short'),\n",
       " (3386, 'many'),\n",
       " (2854, 'gorgeous'),\n",
       " (2675, 'overall'),\n",
       " (2623, 'much'),\n",
       " (2620, 'other')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_small_adj[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9276, 'great'),\n",
       " (8972, 'little'),\n",
       " (8489, 'large'),\n",
       " (7584, 'comfortable'),\n",
       " (7517, 'big'),\n",
       " (3201, 'small'),\n",
       " (3011, 'many'),\n",
       " (2654, 'much'),\n",
       " (2602, 'overall'),\n",
       " (2406, 'smaller')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_big_adj[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Try bigram and trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_adj_n_gram(word_count_dict):\n",
    "\n",
    "    ## get the most common ajectives\n",
    "    adj_count = {\n",
    "        key: word_count_dict[key]\n",
    "        for key in word_count_dict\n",
    "        # if either of the word is an adj, preserve\n",
    "        if any(\n",
    "            wordType[0] == \"J\"\n",
    "            for _, wordType in nltk.pos_tag(key if type(key) == tuple else (key,))\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # sort them into pairs\n",
    "    adj_count_pair = list(adj_count.items())\n",
    "\n",
    "    # # sort\n",
    "    adj_count_pair.sort(reverse=True, key=lambda x: x[1])\n",
    "\n",
    "    return adj_count_pair\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_adj_pipeline(fit_reviews_all, small_reviews_all, large_reviews_all, n=2):\n",
    "\n",
    "    fit_tokenized_all = tokenize_data_sets(fit_reviews_all, n)\n",
    "    small_tokenized_all = tokenize_data_sets(small_reviews_all, n)\n",
    "    large_tokenized_all = tokenize_data_sets(large_reviews_all, n)\n",
    "\n",
    "    fit_word_count = as2_analysis.count_word_freq(fit_tokenized_all)\n",
    "    small_word_count = as2_analysis.count_word_freq(small_tokenized_all)\n",
    "    large_word_count = as2_analysis.count_word_freq(large_tokenized_all)\n",
    "\n",
    "    start = time.time()\n",
    "    top_fit_grams = top_n_adj_n_gram(fit_word_count)\n",
    "    top_small_grams = top_n_adj_n_gram(small_word_count)\n",
    "    top_large_grams = top_n_adj_n_gram(large_word_count)\n",
    "    end = time.time()\n",
    "    print(f\"time consume: {end-start}s\")\n",
    "    return top_fit_grams, top_small_grams, top_large_grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time consume: 187.62095403671265s\n"
     ]
    }
   ],
   "source": [
    "top_fit_grams, top_small_grams, top_large_grams = top_adj_pipeline(\n",
    "    fit_reviews_all, small_reviews_all, large_reviews_all, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', 'little'), 37021),\n",
       " (('was', 'perfect'), 21087),\n",
       " (('many', 'compliments'), 18749),\n",
       " (('true', 'to'), 17248),\n",
       " (('so', 'i'), 16506),\n",
       " (('so', 'many'), 14559),\n",
       " (('comfortable', 'and'), 12829),\n",
       " (('very', 'comfortable'), 11175),\n",
       " (('the', 'top'), 9822),\n",
       " (('a', 'great'), 9118)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "top_fit_grams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', 'little'), 7847),\n",
       " (('so', 'i'), 4534),\n",
       " (('many', 'compliments'), 2749),\n",
       " (('was', 'perfect'), 2652),\n",
       " (('runs', 'small'), 2557),\n",
       " (('the', 'top'), 2184),\n",
       " (('so', 'many'), 2041),\n",
       " (('able', 'to'), 1758),\n",
       " (('beautiful', 'dress'), 1729),\n",
       " (('small', 'i'), 1694)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_small_grams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', 'little'), 8558),\n",
       " (('so', 'i'), 3870),\n",
       " (('was', 'perfect'), 2582),\n",
       " (('the', 'top'), 2401),\n",
       " (('many', 'compliments'), 2366),\n",
       " (('too', 'big'), 2096),\n",
       " (('comfortable', 'and'), 2026),\n",
       " (('runs', 'large'), 1842),\n",
       " (('so', 'many'), 1829),\n",
       " (('very', 'comfortable'), 1735)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_large_grams[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(data_all)\n",
    "valid_percent = 0.2\n",
    "test_percent = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_all[: int(data_size * (1 - valid_percent - test_percent))]\n",
    "data_valid = data_all[\n",
    "    int(data_size * (1 - valid_percent - test_percent)) : int(\n",
    "        data_size * (1 - test_percent)\n",
    "    )\n",
    "]\n",
    "data_test = data_all[int(data_size * (1 - test_percent)) :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_review(data):\n",
    "    fit_reviews = [\n",
    "        d[\"review_text\"] + \" \" + d[\"review_summary\"] for d in data if d[\"fit\"] == \"fit\"\n",
    "    ]\n",
    "    small_reviews = [\n",
    "        d[\"review_text\"] + \" \" + d[\"review_summary\"]\n",
    "        for d in data\n",
    "        if d[\"fit\"] == \"small\"\n",
    "    ]\n",
    "    large_reviews = [\n",
    "        d[\"review_text\"] + \" \" + d[\"review_summary\"]\n",
    "        for d in data\n",
    "        if d[\"fit\"] == \"large\"\n",
    "    ]\n",
    "    return fit_reviews, small_reviews, large_reviews\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Try n-gram BoW logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "fit_reviews_train, small_reviews_train, large_reviews_train = extract_review(data_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time consume: 8.67733097076416s\n"
     ]
    }
   ],
   "source": [
    "top_fit_grams, top_small_grams, top_large_grams = top_adj_pipeline(\n",
    "    fit_reviews_train, small_reviews_train, large_reviews_train, n\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 500\n",
    "top_word_set = {word for word, _ in top_fit_grams[:threshold]}.union(\n",
    "    {word for word, _ in top_small_grams[:threshold]}.union(\n",
    "        {word for word, _ in top_large_grams[:threshold]}\n",
    "    )\n",
    ")\n",
    "wordId = dict(zip(top_word_set, range(len(top_word_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(d, n=1):\n",
    "    feat = [0] * len(top_word_set)\n",
    "    review = d[\"review_text\"] + \" \" + d[\"review_summary\"]\n",
    "    p_list = as2_analysis.tokenize_paragraph(review, n=1)\n",
    "    for word in p_list:\n",
    "        if word not in top_word_set:\n",
    "            continue\n",
    "        # use BoW for now\n",
    "        feat[wordId[word]] += 1\n",
    "    return feat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(data):\n",
    "    return [0 if d[\"fit\"] == \"fit\" else 1 if d[\"fit\"] == \"small\" else 2 for d in data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIT = 0\n",
    "SMALL = 1\n",
    "BIG = 2\n",
    "X_train = [feature(d, n) for d in data_train]\n",
    "y_train = encode_output(data_train)\n",
    "X_valid = [feature(d, n) for d in data_valid]\n",
    "y_valid = encode_output(data_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(C=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.758498013451401"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred == y_valid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
