{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","import time\n","import numpy\n","import urllib\n","import scipy.optimize\n","import random\n","from collections import defaultdict\n","from collections import Counter\n","import string\n","from sklearn import (\n","    linear_model,\n","    naive_bayes,\n","    svm,\n","    metrics,\n","    tree,\n","    neighbors,\n","    decomposition,\n",")\n","import ast\n","import importlib\n","import numpy as np\n","from scipy.spatial import distance\n","import random\n","import nltk\n","import pandas as pd\n","from nltk.corpus import stopwords\n","\n","nltk.download(\"stopwords\")\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys, os\n","\n","sys.path.insert(0, os.path.abspath(\"../cse258_hw/\"))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import as2_analysis_utils as as2_analysis\n","import as2_plot_utils as as2_plot\n","\n","importlib.reload(as2_analysis)\n","importlib.reload(as2_plot)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_all = list(as2_analysis.parseData_line(\"renttherunway_final_data.json\"))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_features(data_all, key):\n","    \"\"\"\n","    return data of particualr key\n","    \"\"\"\n","    return [d[key] for d in data_all]\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["rating_all = extract_features(data_all, \"rating\")\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["null_count = [i == \"null\" for i in rating_all]\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sum(null_count) / len(null_count)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fit_feedbadk = extract_features(data_all, \"fit\")\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fit_feedbadk_freq = Counter(fit_feedbadk)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fit_feedbadk_prct = {key: val / len(data_all) for key, val in fit_feedbadk_freq.items()}\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fit_feedbadk_prct\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Count word frequency"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# top words in fit reviews\n","fit_reviews_all = [\n","    d[\"review_text\"] + \" \" + d[\"review_summary\"] for d in data_all if d[\"fit\"] == \"fit\"\n","]\n","small_reviews_all = [\n","    d[\"review_text\"] + \" \" + d[\"review_summary\"]\n","    for d in data_all\n","    if d[\"fit\"] == \"small\"\n","]\n","large_reviews_all = [\n","    d[\"review_text\"] + \" \" + d[\"review_summary\"]\n","    for d in data_all\n","    if d[\"fit\"] == \"large\"\n","]\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def tokenize_data_sets(dataset, n=1):\n","    \"\"\"\n","    Each item of dataset is a str\n","    \"\"\"\n","    return [\n","        as2_analysis.tokenize_paragraph(\n","            d,\n","            n=n,\n","            remove_stopwrods=True,\n","            stopwords=[\n","                word\n","                for word in stopwords.words(\"english\")\n","                if word != \"not\" and not (\"n'\" in word)\n","            ],\n","        )\n","        for d in dataset\n","    ]\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# tokenize dataset\n","fit_tokenized_all = tokenize_data_sets(fit_reviews_all)\n","small_tokenized_all = tokenize_data_sets(small_reviews_all)\n","large_tokenized_all = tokenize_data_sets(large_reviews_all)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fit_word_count = as2_analysis.count_word_freq(fit_tokenized_all)\n","small_word_count = as2_analysis.count_word_freq(small_tokenized_all)\n","large_word_count = as2_analysis.count_word_freq(large_tokenized_all)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Get most frequent adjective"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def top_n_adj(word_count_dict):\n","    ## get the most common ajectives\n","    adj_count = {\n","        key: word_count_dict[key]\n","        for key in word_count_dict\n","        if nltk.pos_tag([key])[0][1][0] == \"J\"\n","    }\n","\n","    # sort them into pairs\n","    adj_count_pair = [(count, key) for key, count in adj_count.items()]\n","\n","    # sort\n","    adj_count_pair.sort(reverse=True)\n","\n","    return adj_count_pair\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["start = time.time()\n","top_small_adj = top_n_adj(small_word_count)\n","\n","\n","top_big_adj = top_n_adj(large_word_count)\n","\n","\n","## get the most common ajectives\n","adj_fit_count = {\n","    key: fit_word_count[key]\n","    for key in fit_word_count\n","    if nltk.pos_tag([key])[0][1][0] == \"J\"\n","}\n","\n","end = time.time()\n","print(f\"time consume: {end-start}s\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["adj_fit_count_pair = [(count, key) for key, count in adj_fit_count.items()]\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["adj_fit_count_pair.sort(reverse=True)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["adj_fit_count_pair[:10]\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_small_adj[:10]\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_big_adj[:10]\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Try bigram and trigram"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def top_n_adj_n_gram(word_count_dict, adj_set=None):\n","\n","    # if set of adj is given\n","    if adj_set:\n","        adj_count = {\n","            key: word_count_dict[key]\n","            for key in word_count_dict\n","            # if either of the word is an adj, preserve\n","            if key in adj_set\n","        }\n","    else:\n","        ## get the most common ajectives, might be slow\n","        adj_count = {\n","            key: word_count_dict[key]\n","            for key in word_count_dict\n","            # if either of the word is an adj, preserve\n","            if any(\n","                wordType[0] == \"J\"\n","                for _, wordType in nltk.pos_tag(key if type(key) == tuple else (key,))\n","            )\n","        }\n","\n","    # sort them into pairs\n","    adj_count_pair = list(adj_count.items())\n","\n","    # # sort\n","    adj_count_pair.sort(reverse=True, key=lambda x: x[1])\n","\n","    return adj_count_pair\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def top_adj_pipeline(fit_reviews_all, small_reviews_all, large_reviews_all, n=2):\n","\n","    fit_tokenized_all = tokenize_data_sets(fit_reviews_all, n)\n","    small_tokenized_all = tokenize_data_sets(small_reviews_all, n)\n","    large_tokenized_all = tokenize_data_sets(large_reviews_all, n)\n","\n","    fit_word_count = as2_analysis.count_word_freq(fit_tokenized_all)\n","    small_word_count = as2_analysis.count_word_freq(small_tokenized_all)\n","    large_word_count = as2_analysis.count_word_freq(large_tokenized_all)\n","\n","    # all adj for fit, small, and large\n","    fit_adj, small_adj, large_adj = None, None, None\n","\n","    # load if the pickle exists\n","    if os.path.exists(f\"{n}-grams_adj\"):\n","        with open(f\"{n}-grams_adj\", \"rb\") as f:\n","            fit_adj, small_adj, large_adj = pickle.load(f)\n","\n","    start = time.time()\n","    top_fit_grams = top_n_adj_n_gram(fit_word_count, fit_adj)\n","    top_small_grams = top_n_adj_n_gram(small_word_count, small_adj)\n","    top_large_grams = top_n_adj_n_gram(large_word_count, large_adj)\n","    end = time.time()\n","    print(f\"time consume: {end-start}s\")\n","    return top_fit_grams, top_small_grams, top_large_grams\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def write_adj_to_file(n):\n","    # get all the adj from the entire data\n","    top_fit_grams, top_small_grams, top_large_grams = top_adj_pipeline(\n","        fit_reviews_all, small_reviews_all, large_reviews_all, n\n","    )\n","    # make into sets\n","    a, b, c = (\n","        {word for word, _ in top_fit_grams},\n","        {word for word, _ in top_small_grams},\n","        {word for word, _ in top_large_grams},\n","    )\n","    # write to file\n","    with open(f\"./{n}-grams_adj\", \"wb\") as f:\n","        pickle.dump((a, b, c), f)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_fit_grams, top_small_grams, top_large_grams = top_adj_pipeline(\n","    fit_reviews_all, small_reviews_all, large_reviews_all, 3\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_fit_grams[450:500]\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_small_grams[:20]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_large_grams[:20]\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Feature Engineering"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fit_data = [d for d in data_all if d[\"fit\"] == \"fit\"]\n","small_data = [d for d in data_all if d[\"fit\"] == \"small\"]\n","large_data = [d for d in data_all if d[\"fit\"] == \"large\"]\n","data_balanced = random.sample(fit_data, k=len(large_data)) + small_data + large_data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["random.shuffle(data_balanced)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_size = len(data_balanced)\n","valid_percent = 0.2\n","test_percent = 0.2\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","data_train = data_balanced[: int(data_size * (1 - valid_percent - test_percent))]\n","data_valid = data_balanced[\n","    int(data_size * (1 - valid_percent - test_percent)) : int(\n","        data_size * (1 - test_percent)\n","    )\n","]\n","data_test = data_balanced[int(data_size * (1 - test_percent)) :]\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_review(data):\n","    fit_reviews = [\n","        d[\"review_text\"] + \" \" + d[\"review_summary\"] for d in data if d[\"fit\"] == \"fit\"\n","    ]\n","    small_reviews = [\n","        d[\"review_text\"] + \" \" + d[\"review_summary\"]\n","        for d in data\n","        if d[\"fit\"] == \"small\"\n","    ]\n","    large_reviews = [\n","        d[\"review_text\"] + \" \" + d[\"review_summary\"]\n","        for d in data\n","        if d[\"fit\"] == \"large\"\n","    ]\n","    return fit_reviews, small_reviews, large_reviews\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Try n-gram BoW logistic regression/navie bayes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fit_reviews_train, small_reviews_train, large_reviews_train = extract_review(data_train)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["is_tfidf = True\n","\n","ns = {1: 100, 2: 500, 3: 1000, 4: 2000}\n","# ns = {1: 100, 2: 500, 3: 1000, 4: 2000}\n","top_word_set = {\"small\", \"large\", \"big\"}\n","idf_score = {}\n","for n, threshold in ns.items():\n","    top_fit_grams, top_small_grams, top_large_grams = top_adj_pipeline(\n","        fit_reviews_train, small_reviews_train, large_reviews_train, n\n","    )\n","\n","    a, b, c = (\n","        {word for word, _ in top_fit_grams[:threshold]},\n","        {word for word, _ in top_small_grams[:threshold]},\n","        ({word for word, _ in top_large_grams[:threshold]}),\n","    )\n","    top_gram = (a | b | c) - (a & b & c)\n","    # top_gram = a | b | c\n","    if is_tfidf:\n","        idf_score.update(\n","            as2_analysis.calcualte_idf_score(\n","                top_gram,\n","                tokenize_data_sets(\n","                    fit_reviews_train + small_reviews_train + large_reviews_train, n\n","                ),\n","            )\n","        )\n","    print(f\"unique top {n}-grams: {len(top_gram)}\")\n","    top_word_set |= top_gram\n","\n","print(f\"unique top grams: {len(top_word_set)}\")\n","\n","wordId = dict(zip(top_word_set, range(len(top_word_set))))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def feature(d, ns=[1], is_tfidf=False):\n","    feat = [0] * len(top_word_set)\n","    review = d[\"review_text\"] + \" \" + d[\"review_summary\"]\n","    p_list = []\n","    for n in ns:\n","        p_list.extend(\n","            as2_analysis.tokenize_paragraph(\n","                review,\n","                n=n,\n","                remove_stopwrods=True,\n","                stopwords=[\n","                    word\n","                    for word in stopwords.words(\"english\")\n","                    if word != \"not\" and not (\"n'\" in word)\n","                ],\n","            )\n","        )\n","    for word in p_list:\n","        if word not in top_word_set:\n","            continue\n","        # use BoW for now\n","        feat[wordId[word]] += 1\n","    if is_tfidf:\n","        for word, score in idf_score.items():\n","            feat[wordId[word]] *= score\n","\n","    return feat\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def encode_output(data):\n","    return [labels[d[\"fit\"]] for d in data]\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","labels = {\"fit\": 0, \"small\": 1, \"large\": 2}\n","X_train = [feature(d, ns, is_tfidf) for d in data_train]\n","y_train = encode_output(data_train)\n","X_valid = [feature(d, ns, is_tfidf) for d in data_valid]\n","y_valid = encode_output(data_valid)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" # Modeling"]},{"cell_type":"markdown","metadata":{},"source":[" Try the following numerous text classfication algorithm:\n"," \n","    - Navie Bayes\n","    - SVM\n","    - Logistic Regression\n","    - Decision Tree\n","    - KNN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["models_data = {}"]},{"cell_type":"markdown","metadata":{},"source":[" ## Navie Bayes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = naive_bayes.MultinomialNB()\n","model.fit(X_train, y_train)\n","y_pred = model.predict(X_valid)\n","print(\"Navie Bayes classifcation report\\n\")\n","as2_plot.plot_cm(\n","    metrics.confusion_matrix(y_valid, y_pred, normalize=\"true\"), list(labels.keys())\n",")\n","print(metrics.classification_report(y_valid, y_pred))\n","print()\n","models_data[\"naive bayes\"] = metrics.classification_report(\n","    y_valid, y_pred, output_dict=1\n",")\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Try dimensionality reduction for more time consuimg algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["svd = decomposition.TruncatedSVD(n_components=50)\n","X_train_reduce = svd.fit_transform(X_train)\n","X_valid_reduce = svd.transform(X_valid)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","regs = [0.1, 1, 10, 100]\n","weights = [None]\n","models = {\n","    f\"C={reg}, weights={weight}\": linear_model.LogisticRegression(\n","        C=reg, class_weight=weight\n","    )\n","    for reg in regs\n","    for weight in weights\n","}\n","\n","\n","for desc, model in models.items():\n","    model.fit(X_train_reduce, y_train)\n","    y_pred = model.predict(X_valid_reduce)\n","    print(f\"Logistic Regression {desc} classifcation report\\n\")\n","    as2_plot.plot_cm(\n","        metrics.confusion_matrix(y_valid, y_pred, normalize=\"true\"), list(labels.keys())\n","    )\n","    print(metrics.classification_report(y_valid, y_pred))\n","    print()\n","    models_data[f\"logistic regression, {desc}\"] = metrics.classification_report(\n","        y_valid, y_pred, output_dict=1\n","    )\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":["## SVM\n","\n","Try the following kernel:\n","\n","    - linear\n","    - rbf\n","    - polynomial"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# svd = decomposition.TruncatedSVD(n_components=50)\n","# X_train_reduce = svd.fit_transform(X_train)\n","# X_valid_reduce = svd.transform(X_valid)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","regs = [0.1, 1, 10]\n","models = {}\n","models.update({f\"kernel=linear, C={reg}\": svm.LinearSVC(C=reg) for reg in regs})\n","for desc, model in models.items():\n","    model.fit(X_train_reduce, y_train)\n","    y_pred = model.predict(X_valid_reduce)\n","    print(f\"SVM {desc} classifcation report and confusion matrix\\n\")\n","    as2_plot.plot_cm(\n","        metrics.confusion_matrix(y_valid, y_pred, normalize=\"true\"), list(labels.keys())\n","    )\n","    print(metrics.classification_report(y_valid, y_pred))\n","    print()\n","    models_data[f\"svm, {desc}\"] = metrics.classification_report(\n","        y_valid, y_pred, output_dict=1\n","    )\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["svd = decomposition.TruncatedSVD(n_components=100)\n","X_train_reduce = svd.fit_transform(X_train)\n","X_valid_reduce = svd.transform(X_valid)\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["depths = [5, 10, 20, 50]\n","models = {\n","    f\"max_depth={depth}\": tree.DecisionTreeClassifier(max_depth=depth)\n","    for depth in depths\n","}\n","\n","\n","for desc, model in models.items():\n","    # delay?\n","    model.fit(X_train_reduce, y_train)\n","    y_pred = model.predict(X_valid_reduce)\n","    print(f\"Decision Tree {desc} classifcation report\\n\")\n","    as2_plot.plot_cm(\n","        metrics.confusion_matrix(y_valid, y_pred, normalize=\"true\"), list(labels.keys())\n","    )\n","    print(metrics.classification_report(y_valid, y_pred))\n","    print()\n","    models_data[f\"decision tree, {desc}\"] = metrics.classification_report(\n","        y_valid, y_pred, output_dict=1\n","    )\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## K-nearest neighbor"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["neighbor_nums = [20, 50, 100, 300]\n","models = {\n","    f\"n-neighbor={neighbor_num}\": neighbors.KNeighborsClassifier(\n","        n_neighbors=neighbor_num\n","    )\n","    for neighbor_num in neighbor_nums\n","}\n","for desc, model in models.items():\n","    model.fit(X_train_reduce, y_train)\n","    y_pred = model.predict(X_valid_reduce)\n","    print(f\"KNN {desc} classifcation report\\n\")\n","    as2_plot.plot_cm(\n","        metrics.confusion_matrix(y_valid, y_pred, normalize=\"true\"), list(labels.keys())\n","    )\n","    print(metrics.classification_report(y_valid, y_pred))\n","    print()\n","    models_data[f\"knn, {desc}\"] = metrics.classification_report(\n","        y_valid, y_pred, output_dict=1\n","    )\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}